# Nano Model Configuration
# Designed for instant training and testing

# Architecture (Tiny)
vocab_size=256 # Byte-level tokenizer (0-255)
hidden_dim=64
mem_dim=64
ffn_dim=128
num_layers=1
num_experts=1
top_k=1

# Training
batch_size=8
max_seq_len=32
learning_rate=0.00001  # Reduced to 1e-5 (Flux Base) to fix divergence (PPL 75)
# optimizer_type=sgd
optimizer_type=flux
weight_decay=0.01     # Added weight decay
grad_clip_norm=1.0    # Added gradient clipping
warmup_steps=100      # Added warmup for stability
max_iterations=1000 # epochs

# Adaptive (Flux) - relaxed for nano
hard_threshold=10.0
easy_threshold=0.0   # Disabled: Train on everything. Only skip Broken/Unlearnable (>10).

# System
checkpoint_path=nano_ckpt.bin
