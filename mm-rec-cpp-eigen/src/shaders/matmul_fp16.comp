#version 450
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require

layout(local_size_x = 16, local_size_y = 16) in;

// Binding 0: Matrix A (M x K) - readonly
// Note: CPU sends float (32-bit). We read as float and cast? 
// OR we should pack data as float16 on CPU?
// For simplicity in this step, let's keep Input as float32 but compute in float16.
// This is "Mixed Precision".
// To get Memory Bandwidth gain, we MUST pack inputs as float16 on CPU.
// But that requires `_Float16` support on C++ side or manual packing.
// Let's start with Mixed Precision: Read float -> Convert f16 -> Compute f16 -> Write float.
// This tests ALU speedup but not Memory Bandwidth speedup.
// Better: Cast to float16 inside shared memory.

layout(binding = 0) readonly buffer InputA {
    float dataA[];
};

layout(binding = 1) readonly buffer InputB {
    float dataB[];
};

layout(binding = 2) writeonly buffer OutputC {
    float dataC[];
};

layout(push_constant) uniform PushConsts {
    int M;
    int N;
    int K;
} params;

// Block size
const int TS = 16;

// Shared memory: Use float16_t to save shared memory bank bandwidth?
// Yes.
shared float16_t As[TS][TS];
shared float16_t Bs[TS][TS];

void main() {
    int tx = int(gl_LocalInvocationID.x);
    int ty = int(gl_LocalInvocationID.y);
    int row = int(gl_GlobalInvocationID.y);
    int col = int(gl_GlobalInvocationID.x);

    float16_t acc = float16_t(0.0);

    int numTiles = (params.K + TS - 1) / TS;

    for (int t = 0; t < numTiles; ++t) {
        // Load A
        int r = row;
        int c = t * TS + tx;
        if (r < params.M && c < params.K) {
            As[ty][tx] = float16_t(dataA[r * params.K + c]); // Downcast
        } else {
            As[ty][tx] = float16_t(0.0);
        }

        // Load B
        r = t * TS + ty;
        c = col;
        if (r < params.K && c < params.N) {
            Bs[ty][tx] = float16_t(dataB[r * params.N + c]); // Downcast
        } else {
            Bs[ty][tx] = float16_t(0.0);
        }

        barrier();

        // Compute in FP16
        for (int k = 0; k < TS; ++k) {
            acc += As[ty][k] * Bs[k][tx];
        }

        barrier();
    }

    if (row < params.M && col < params.N) {
        dataC[row * params.N + col] = float(acc); // Upcast
    }
}
