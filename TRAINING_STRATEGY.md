# MM-Rec Progressive Training Stratejisi

**Tarih**: 2025-01-27  
**AmaÃ§**: En kÃ¼Ã§Ã¼k temel modelden baÅŸlayarak bÃ¼yÃ¼k modellere evrilme ve uzmanlÄ±k alanlarÄ±yla fine-tuning

---

## ğŸ¯ Strateji Ã–zeti

### 1. En KÃ¼Ã§Ã¼k Temel Model (BaÅŸlangÄ±Ã§)

**Tiny Base Model**:
- Parametre: **230K** (0.23M)
- Bellek (FP16): **0.44 MB**
- KonfigÃ¼rasyon:
  - Vocab: 5,000
  - Model Dim: 128
  - Layers: 4
  - Heads: 4
  - Max Seq Len: 1,024

**Neden Bu Model?**:
- âœ… Ã‡ok hÄ±zlÄ± eÄŸitilir (test ve doÄŸrulama iÃ§in)
- âœ… Minimum kaynak gerektirir
- âœ… Architecture'Ä± doÄŸrulamak iÃ§in ideal
- âœ… Progressive training'in temelini oluÅŸturur

### 2. Progressive Training Sequence

```
Tiny (0.23M) 
  â†“ (Weight Transfer + Training)
Mini (2M)
  â†“ (Weight Transfer + Training)
Small (10M)
  â†“ (Weight Transfer + Training)
Base (52M)
  â†“ (Weight Transfer + Training)
Medium (200M+)
  â†“ (Weight Transfer + Training)
Large (500M+)
  â†“ (Weight Transfer + Training)
7B (7.38B)
```

### 3. Her Stage'de YapÄ±lacaklar

1. **Weight Transfer**: Ã–nceki modelden weight'leri transfer et
2. **Training**: Yeni model boyutunda eÄŸitim
3. **Checkpointing**: Model checkpoint kaydet
4. **Fine-tuning (Opsiyonel)**: UzmanlÄ±k alanlarÄ±yla fine-tune et

---

## ğŸ“‹ Model KonfigÃ¼rasyonlarÄ±

### Tiny Base (BaÅŸlangÄ±Ã§)
```python
vocab_size=5000
model_dim=128
num_layers=4
num_heads=4
max_seq_len=1024
use_hem=True
use_dpg=False
use_uboo=False
```

### Mini Base
```python
vocab_size=10000
model_dim=256
num_layers=6
num_heads=4
max_seq_len=2048
use_hem=True
use_dpg=False
use_uboo=False
```

### Small Base
```python
vocab_size=20000
model_dim=512
num_layers=8
num_heads=8
max_seq_len=4096
use_hem=True
use_dpg=False
use_uboo=False
```

### Base Base
```python
vocab_size=32000
model_dim=1024
num_layers=12
num_heads=8
max_seq_len=8192
use_hem=True
use_dpg=False
use_uboo=False
```

### Medium Base
```python
vocab_size=32000
model_dim=2048
num_layers=16
num_heads=16
max_seq_len=16384
use_hem=True
use_dpg=True  # DPG aktif
use_uboo=False
```

### Large Base
```python
vocab_size=32000
model_dim=3072
num_layers=20
num_heads=24
max_seq_len=32768
use_hem=True
use_dpg=True
use_uboo=True  # UBÃ–O aktif
```

### 7B (Hedef)
```python
vocab_size=32000
model_dim=4096
num_layers=24
num_heads=32
max_seq_len=32768
use_hem=True
use_dpg=True
use_uboo=True
```

---

## ğŸš€ KullanÄ±m

### 1. En KÃ¼Ã§Ã¼k Temel Modeli EÄŸitme

```bash
# Tiny model eÄŸitimi
python mm_rec/scripts/train_base_model.py \
    --config tiny \
    --output-dir checkpoints \
    --epochs 10 \
    --batch-size 4 \
    --seq-len 512 \
    --lr 3e-4
```

### 2. Progressive Training (Tiny -> 7B)

```bash
# Progressive training (tÃ¼m sequence)
python mm_rec/scripts/train_base_model.py \
    --progressive \
    --start-config tiny \
    --end-config 7b \
    --epochs-per-stage 5 \
    --output-dir checkpoints
```

### 3. Belirli Stage'den Devam

```bash
# Mini model eÄŸitimi (Tiny'dan sonra)
python mm_rec/scripts/train_base_model.py \
    --config mini \
    --output-dir checkpoints \
    --epochs 10 \
    --resume-from checkpoints/tiny/final_checkpoint.pt
```

### 4. Expert Fine-tuning

```bash
# Pretrained model'i uzmanlÄ±k alanÄ±yla fine-tune et
python mm_rec/scripts/finetune_expert.py \
    --checkpoint checkpoints/base/final_checkpoint.pt \
    --expert-name medical \
    --output-dir experts \
    --epochs 5 \
    --lr 1e-5
```

---

## ğŸ”„ Weight Transfer Stratejisi

### AkÄ±llÄ± Weight Transfer

1. **Embedding**: Ortak vocab kÄ±smÄ± transfer edilir
2. **Blocks**: Mevcut layer'lar transfer edilir, yeni layer'lar random init
3. **Layer Norm**: AynÄ± dimension'lar transfer edilir
4. **Output Head**: Ortak vocab kÄ±smÄ± transfer edilir

### Ã–rnek: Tiny -> Mini Transfer

```python
from mm_rec.utils.model_upscaling import upscale_model
from mm_rec.configs.base_model_configs import TINY_BASE_CONFIG, MINI_BASE_CONFIG

# Tiny model yÃ¼kle
tiny_model = load_checkpoint("checkpoints/tiny/final_checkpoint.pt")

# Mini model'e upscale et
mini_model = upscale_model(
    tiny_model,
    MINI_BASE_CONFIG,
    device=device
)

# Mini model'i eÄŸit
train_model(mini_model, ...)
```

---

## ğŸ“Š Fine-tuning Stratejisi

### Expert Model OluÅŸturma

1. **Base Model YÃ¼kle**: Pretrained base model'i yÃ¼kle
2. **Freeze Layers**: Ä°lk N layer'Ä± freeze et (knowledge preservation)
3. **Train Last Layers**: Son M layer'Ä± train et (task adaptation)
4. **Low Learning Rate**: Fine-tuning iÃ§in dÃ¼ÅŸÃ¼k LR (1e-5)

### Ã–rnek Expert AlanlarÄ±

- **Medical**: TÄ±bbi metinler
- **Code**: Kod Ã¼retimi
- **Math**: Matematik problemleri
- **Legal**: Hukuki metinler
- **Finance**: Finansal analiz

---

## âœ… Avantajlar

1. **HÄ±zlÄ± Ä°terasyon**: KÃ¼Ã§Ã¼k modelle hÄ±zlÄ± test
2. **Kaynak VerimliliÄŸi**: Progressive training kaynak tasarrufu saÄŸlar
3. **Knowledge Transfer**: KÃ¼Ã§Ã¼k modelden bÃ¼yÃ¼k modele bilgi transferi
4. **Esneklik**: Her stage'de fine-tuning yapÄ±labilir
5. **Scalability**: 7B'ye kadar Ã¶lÃ§eklenebilir

---

## ğŸ“ Sonraki AdÄ±mlar

1. âœ… En kÃ¼Ã§Ã¼k temel model (Tiny) eÄŸitimi
2. â³ Weight transfer mekanizmasÄ± testi
3. â³ Progressive training pipeline
4. â³ Expert fine-tuning altyapÄ±sÄ±
5. â³ GerÃ§ek dataset entegrasyonu

---

**HazÄ±rlayan**: MM-Rec Training Team  
**Tarih**: 2025-01-27
