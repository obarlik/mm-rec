# MM-Rec 100M ModÃ¼ler EÄŸitim ve MLOps TasarÄ±m DokÃ¼mantasyonu

**Versiyon**: 1.0  
**Tarih**: 2025-12-08  
**Hedef**: 100M parametreli, 256 kanallÄ±, ModÃ¼ler MM-Rec modeli ve MLOps hattÄ±

---

## ðŸ“‹ Ä°Ã§indekiler

1. [Model Mimarisi](#1-model-mimarisi)
2. [Hibrit Hassasiyet Implementasyonu](#2-hibrit-hassasiyet-implementasyonu)
3. [Session-Based Memory Management](#3-session-based-memory-management)
4. [ModÃ¼ler EÄŸitim Stratejisi](#4-modÃ¼ler-eÄŸitim-stratejisi)
5. [Fusion Layer](#5-fusion-layer)
6. [MLOps Orkestrasyonu](#6-mlops-orkestrasyonu)
7. [Monitoring ve Recovery](#7-monitoring-ve-recovery)
8. [DoÄŸrulama ve Testler](#8-doÄŸrulama-ve-testler)

---

## 1. Model Mimarisi

### 1.1 MM-Rec 100M Model YapÄ±sÄ±

**KonfigÃ¼rasyon**:
- **Parametre SayÄ±sÄ±**: ~100M
- **Expert Dimension**: 256 (her expert)
- **Expert SayÄ±sÄ±**: 2 (Text Expert + Code Expert)
- **Fusion Dimension**: 512 (256 + 256)
- **Layer SayÄ±sÄ±**: 16 per expert
- **FFN Dimension**: 3072 per expert
- **Vocab Size**: 32,000

**Mimari**:
```
Input [batch, seq_len]
  â†“
Embedding [batch, seq_len, 256]
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Expert    â”‚  Code Expert    â”‚
â”‚  (256 channels) â”‚  (256 channels) â”‚
â”‚  16 layers      â”‚  16 layers      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Fusion Layer (512)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Final Norm + LM Head
  â†“
Output [batch, seq_len, vocab_size]
```

### 1.2 Expert ModÃ¼lleri

**Text Expert**:
- 16 MMRecBlock layers
- 256 model dimension
- 3072 FFN dimension
- 8 attention heads
- Kendi memory state'leri (M_text, h_t_text)

**Code Expert**:
- 16 MMRecBlock layers
- 256 model dimension
- 3072 FFN dimension
- 8 attention heads
- Kendi memory state'leri (M_code, h_t_code)

### 1.3 Dosya YapÄ±sÄ±

```
mm_rec/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mmrec_100m.py          # MMRec100M model
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ associative_scan_hybrid.py  # BF16 + FP64 hybrid
â”‚   â””â”€â”€ session_memory.py      # Session-based memory management
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_modular.py       # ModÃ¼ler eÄŸitim script'i
â””â”€â”€ utils/
    â””â”€â”€ monitoring.py           # Monitoring hooks
```

---

## 2. Hibrit Hassasiyet Implementasyonu

### 2.1 Hassasiyet Stratejisi

**BF16 (Bfloat16)**:
- Model weights
- Activations
- Standard operations

**FP64 (Double Precision)**:
- **Sadece**: Log-space cumulative sum (prefix sum) in associative scan
- Kritik sayÄ±sal stabilite iÃ§in

### 2.2 Implementasyon

**Dosya**: `mm_rec/core/associative_scan_hybrid.py`

```python
class HybridPrecisionAssociativeScan:
    @staticmethod
    def forward(gamma: torch.Tensor) -> torch.Tensor:
        # 1. Convert to FP32 for log operations
        gamma_fp32 = gamma.to(torch.float32)
        
        # 2. Convert to log-space (FP32)
        log_gamma = torch.log(gamma_fp32 + 1e-8)
        log_gamma = torch.clamp(log_gamma, -50.0, 0.0)
        
        # 3. CRITICAL: Convert to FP64 for accumulation
        log_gamma_fp64 = log_gamma.to(torch.float64)
        
        # 4. Cumulative sum in FP64 (double precision)
        log_cumsum_fp64 = torch.cumsum(log_gamma_fp64, dim=2)
        
        # 5. Convert back to FP32 for exp
        log_cumsum_fp32 = log_cumsum_fp64.to(torch.float32)
        
        # 6. Stable exponential conversion
        max_log = torch.max(log_cumsum_fp32, dim=2, keepdim=True)[0]
        stable_log = log_cumsum_fp32 - max_log
        cumulative_product = torch.exp(stable_log) * torch.exp(max_log)
        
        # 7. Convert back to BF16
        return cumulative_product.to(gamma.dtype)
```

**KullanÄ±m**:
```python
from mm_rec.core.associative_scan_hybrid import associative_scan_exponential_hybrid

# BF16 input
gamma = torch.rand(2, 8, 1024, 64, dtype=torch.bfloat16, device='cuda')

# Hybrid precision scan (BF16 â†’ FP64 log accumulation â†’ BF16)
result = associative_scan_exponential_hybrid(gamma)
```

---

## 3. Session-Based Memory Management

### 3.1 Session Memory Manager

**Dosya**: `mm_rec/core/session_memory.py`

**Ã–zellikler**:
- Session ID bazlÄ± memory state yÃ¶netimi
- Serialize/load memory states (M ve h_t)
- File-based veya database-based storage
- I/O optimizasyonu

**KullanÄ±m**:
```python
from mm_rec.core.session_memory import SessionMemoryManager

# Initialize manager
manager = SessionMemoryManager(base_dir="./memory_sessions")

# Serialize memory states
session_id = "user_123_session_1"
memory_states = {
    "text": [text_expert_states],
    "code": [code_expert_states]
}
save_path = manager.serialize_state(session_id, memory_states)

# Load memory states
loaded_states = manager.load_state(session_id, device=device)
```

### 3.2 Memory State Structure

**Short-term Memory (h_t)**:
- Per-timestep hidden states
- Shape: [batch, seq_len, model_dim]
- Updated sequentially

**Long-term Memory (M)**:
- Persistent memory matrix
- Shape: [batch, num_memories, M, mem_dim] where M=1024
- Updated incrementally

**Serialization Format**:
```
memory_sessions/
â””â”€â”€ {session_id}/
    â”œâ”€â”€ text/
    â”‚   â”œâ”€â”€ layer_0.pt
    â”‚   â”œâ”€â”€ layer_1.pt
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ code/
    â”‚   â”œâ”€â”€ layer_0.pt
    â”‚   â”œâ”€â”€ layer_1.pt
    â”‚   â””â”€â”€ ...
    â””â”€â”€ metadata.json
```

---

## 4. ModÃ¼ler EÄŸitim Stratejisi

### 4.1 Kademeli EÄŸitim PlanÄ±

**Stage 1: Local Consistency (Lokal TutarlÄ±lÄ±k)**
- **AmaÃ§**: MDI kapÄ±larÄ±nÄ±n kÄ±sa sekanslarla lokal tutarlÄ±lÄ±k kazanmasÄ±
- **Sequence Length**: 512 tokens
- **EÄŸitim**: Her iki expert birlikte
- **Fusion**: KullanÄ±lmaz
- **Learning Rate**: 3e-4
- **Steps**: 5,000

**Stage 2: Global Specialization (Global UzmanlaÅŸma)**
- **AmaÃ§**: UzmanlarÄ±n domain-specific uzun sekanslarda Ã§alÄ±ÅŸmasÄ±
- **Sequence Length**: 8,192 tokens
- **EÄŸitim**: Uzmanlar ayrÄ± ayrÄ± (Text â†’ Code)
- **Fusion**: KullanÄ±lmaz
- **Learning Rate**: 1e-4
- **Steps**: 10,000

**Stage 3: Fusion Training (FÃ¼zyon EÄŸitimi)**
- **AmaÃ§**: Fusion layer'Ä±n uzmanlarÄ± birleÅŸtirmeyi Ã¶ÄŸrenmesi
- **Sequence Length**: 4,096 tokens
- **EÄŸitim**: Her iki expert + fusion layer
- **Fusion**: KullanÄ±lÄ±r
- **Learning Rate**: 5e-5
- **Steps**: 5,000

### 4.2 EÄŸitim Script'i

**Dosya**: `mm_rec/scripts/train_modular.py`

**KullanÄ±m**:
```bash
# Stage 1 only
python -m mm_rec.scripts.train_modular --stage stage1

# Stage 2 only
python -m mm_rec.scripts.train_modular --stage stage2

# Stage 3 only
python -m mm_rec.scripts.train_modular --stage stage3

# All stages
python -m mm_rec.scripts.train_modular --stage all
```

**Checkpointing**:
- Her stage iÃ§in ayrÄ± checkpoint'ler
- Format: `checkpoint_{stage}_step_{step}.pt`
- Resume: `--resume_from checkpoint_path`

---

## 5. Fusion Layer

### 5.1 Fusion MekanizmasÄ±

**AmaÃ§**: Ä°ki 256-kanallÄ± expert'i 512 kanallÄ± birleÅŸik hafÄ±za alanÄ±na dÃ¶nÃ¼ÅŸtÃ¼rme

**YÃ¶ntemler**:
1. **Concatenate**: Basit birleÅŸtirme (256 + 256 = 512)
2. **Weighted**: Ã–ÄŸrenilebilir aÄŸÄ±rlÄ±klÄ± birleÅŸtirme

**Implementasyon**:
```python
class FusionLayer(nn.Module):
    def forward(self, text_output, code_output, text_memory=None, code_memory=None):
        # Concatenate expert outputs
        combined = torch.cat([text_output, code_output], dim=-1)  # [batch, seq_len, 512]
        
        # Apply fusion projection
        fused = self.fusion_proj(combined)  # [batch, seq_len, 512]
        
        return fused
```

**Inference KullanÄ±mÄ±**:
```python
# EÄŸitilmiÅŸ expert'lerden Ã§Ä±ktÄ± al
text_output, text_states = model.text_expert(x, text_memory_states)
code_output, code_states = model.code_expert(x, code_memory_states)

# Fusion layer ile birleÅŸtir
fused = model.fusion(text_output, code_output, text_memory, code_memory)
# fused: [batch, seq_len, 512]
```

---

## 6. MLOps Orkestrasyonu

### 6.1 Dockerfile

**Dosya**: `mlops/Dockerfile`

**Ã–zellikler**:
- CUDA 11.8 + cuDNN 8
- PyTorch with CUDA support
- Triton kernel support
- TÃ¼m baÄŸÄ±mlÄ±lÄ±klar

**Build**:
```bash
docker build -t mmrec-100m:latest -f mlops/Dockerfile .
```

### 6.2 Kubernetes Deployment

**Dosya**: `mlops/kubernetes/job.yaml`

**Ã–zellikler**:
- GPU resource requests/limits
- Persistent volume claims (checkpoints, data)
- Environment variables
- Resource limits

**Deploy**:
```bash
kubectl apply -f mlops/kubernetes/job.yaml
```

### 6.3 Slurm Script

**Dosya**: `mlops/slurm/train.sh`

**Ã–zellikler**:
- GPU allocation
- Resource limits
- Logging

**Submit**:
```bash
sbatch mlops/slurm/train.sh
```

---

## 7. Monitoring ve Recovery

### 7.1 Monitoring Hooks

**Dosya**: `mm_rec/utils/monitoring.py`

**Ã–zellikler**:

1. **Numerical Stability Monitor**:
   - NaN/Inf detection in outputs
   - Memory state stability checks
   - Alert threshold (3 consecutive issues)

2. **Memory Norm Tracker**:
   - Track M and h_t norms
   - Detect sudden changes (>10x ratio)
   - Log history

**KullanÄ±m**:
```python
from mm_rec.utils.monitoring import create_monitoring_hooks

hooks = create_monitoring_hooks(model, checkpoint_dir="./checkpoints")

# In training loop
for step in range(num_steps):
    # Forward pass
    logits, memory_states = model(input_ids, return_memory=True)
    
    # Check outputs
    hooks["stability_monitor"].check_outputs({"logits": logits}, step)
    
    # Track memory norms
    hooks["memory_hook"](memory_states, step)
```

### 7.2 Recovery Protocol

**Dosya**: `mlops/recovery_protocol.sh`

**Ã–zellikler**:
- Otomatik checkpoint bulma
- Learning rate reduction (0.5x)
- Training resume
- Maximum recovery attempts (3)

**KullanÄ±m**:
```bash
# Manual recovery
./mlops/recovery_protocol.sh

# Automatic (from monitoring)
if numerical_error_detected:
    recovery.recover(model, optimizer, error_type="numerical_error")
```

**Recovery Steps**:
1. Find latest checkpoint
2. Load model and optimizer states
3. Reduce learning rate by 0.5x
4. Resume training
5. Log recovery attempt

---

## 8. DoÄŸrulama ve Testler

### 8.1 Associative Scan Validation

**Dosya**: `mm_rec/tests/test_associative_scan_validation.py`

**Testler**:
- Short sequence (128 tokens)
- Medium sequence (1024 tokens)
- Long sequence (8192 tokens)
- Hybrid precision test
- Numerical stability test (extreme values)

**Ã‡alÄ±ÅŸtÄ±rma**:
```bash
python -m pytest mm_rec/tests/test_associative_scan_validation.py -v
```

**DoÄŸrulama**:
- Triton kernel output vs sequential Python implementation
- Tolerance: rtol=1e-3, atol=1e-4
- NaN/Inf checks

### 8.2 32K Sequence Test

**Dosya**: `mm_rec/tests/test_32k_sequence.py`

**Testler**:
- 32K forward pass
- 32K with memory states
- Chunking consistency (different chunk sizes)

**Ã‡alÄ±ÅŸtÄ±rma**:
```bash
python -m pytest mm_rec/tests/test_32k_sequence.py -v
```

**DoÄŸrulama**:
- Model processes 32,768 tokens without errors
- Output shape correctness
- No NaN/Inf in outputs
- Chunking produces consistent results

---

## 9. KullanÄ±m Ã–rnekleri

### 9.1 Model OluÅŸturma

```python
from mm_rec.models.mmrec_100m import MMRec100M

model = MMRec100M(
    vocab_size=32000,
    expert_dim=256,
    num_layers=16,
    num_heads=8,
    ffn_dim=3072
)

print(f"Parameters: {model.get_num_params():,}")  # ~100M
```

### 9.2 ModÃ¼ler EÄŸitim

```bash
# Stage 1: Local consistency
python -m mm_rec.scripts.train_modular \
    --stage stage1 \
    --batch_size 4 \
    --checkpoint_dir ./checkpoints

# Stage 2: Global specialization
python -m mm_rec.scripts.train_modular \
    --stage stage2 \
    --batch_size 2 \
    --checkpoint_dir ./checkpoints

# Stage 3: Fusion training
python -m mm_rec.scripts.train_modular \
    --stage stage3 \
    --batch_size 4 \
    --checkpoint_dir ./checkpoints
```

### 9.3 Session Memory Management

```python
from mm_rec.core.session_memory import SessionMemoryManager

manager = SessionMemoryManager(base_dir="./memory_sessions")

# Save state
session_id = "user_123"
memory_states = {
    "text": text_expert_states,
    "code": code_expert_states
}
manager.serialize_state(session_id, memory_states)

# Load state
loaded_states = manager.load_state(session_id, device=device)
```

### 9.4 Inference with Fusion

```python
# Load trained model
model.load_state_dict(torch.load("checkpoint_stage3_final.pt"))

# Inference
input_ids = torch.randint(0, 32000, (1, 4096), device=device)

# Use both experts and fuse
logits = model(input_ids, expert_type=None)  # Automatic fusion

# Or use single expert
text_logits = model(input_ids, expert_type="text")
code_logits = model(input_ids, expert_type="code")
```

---

## 10. Performans Beklentileri

### 10.1 Model Boyutu

- **Parameters**: ~100M
- **BF16 Size**: ~200 MB
- **FP32 Size**: ~400 MB
- **Training Memory**: ~2-4 GB (with gradient checkpointing)

### 10.2 EÄŸitim SÃ¼resi (Tahmini)

- **Stage 1**: ~2-4 hours (5K steps, seq_len=512)
- **Stage 2**: ~8-12 hours (10K steps, seq_len=8192)
- **Stage 3**: ~2-4 hours (5K steps, seq_len=4096)
- **Total**: ~12-20 hours (single GPU)

### 10.3 Inference

- **32K sequence**: ~2-4 seconds (with chunking)
- **Memory usage**: ~2-3 GB
- **Throughput**: ~8K-16K tokens/second

---

## 11. Kritik Notlar

### 11.1 Mutlaka YapÄ±lmasÄ± Gerekenler

1. **FP64 Log Accumulation**: Associative scan'de mutlaka kullan
2. **Session Memory**: Her session iÃ§in ayrÄ± memory state yÃ¶net
3. **Staged Training**: Stage 1 â†’ 2 â†’ 3 sÄ±rasÄ±nÄ± takip et
4. **Monitoring**: NaN/Inf detection'Ä± aktif tut
5. **Checkpointing**: Her stage'de dÃ¼zenli checkpoint al

### 11.2 YapÄ±lmamasÄ± Gerekenler

1. **Direct FP64**: TÃ¼m model'i FP64 yapma (sadece log accumulation)
2. **Skip Stages**: Stage'leri atlama
3. **Ignore Monitoring**: Monitoring hook'larÄ±nÄ± kapatma
4. **No Recovery**: Recovery protocol'Ã¼ devre dÄ±ÅŸÄ± bÄ±rakma

---

## 12. SonuÃ§

MM-Rec 100M modÃ¼ler model ve MLOps hattÄ±, aÅŸaÄŸÄ±daki Ã¶zellikleri saÄŸlar:

âœ… **100M parametreli modÃ¼ler model** (2 experts, 256 channels each)  
âœ… **Hibrit hassasiyet** (BF16 + FP64 log accumulation)  
âœ… **Session-based memory management**  
âœ… **Kademeli modÃ¼ler eÄŸitim** (3 stage)  
âœ… **Fusion layer** (512 channels from 2x256)  
âœ… **MLOps orkestrasyonu** (Docker, Kubernetes, Slurm)  
âœ… **Monitoring ve recovery** (NaN/Inf detection, automatic recovery)  
âœ… **DoÄŸrulama testleri** (kernel validation, 32K sequence)

**DokÃ¼mantasyon Versiyonu**: 1.0  
**Son GÃ¼ncelleme**: 2025-12-08

