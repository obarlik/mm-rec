# MM-Rec 7B Model - Parametre SayÄ±sÄ± Analizi

**Tarih**: 2025-12-08  
**Model KonfigÃ¼rasyonu**: 7B (7.38B parameters)

---

## ğŸ“Š Toplam Parametre SayÄ±sÄ±

**7,380,553,728 parametre** (â‰ˆ **7.38 B**)

### Model Boyutu
- **BF16**: 13.75 GB
- **FP32**: 27.49 GB

---

## ğŸ“‹ DetaylÄ± Parametre DaÄŸÄ±lÄ±mÄ±

### 1. Embedding Layer
- **Parametreler**: 131,072,000
- **Hesaplama**: `vocab_size Ã— model_dim = 32,000 Ã— 4,096`
- **Not**: LM Head ile tied (aynÄ± aÄŸÄ±rlÄ±klar paylaÅŸÄ±lÄ±yor)

### 2. LM Head
- **Parametreler**: 131,072,000 (Embedding ile tied - tekrar sayÄ±lmÄ±yor)
- **Hesaplama**: `model_dim Ã— vocab_size = 4,096 Ã— 32,000`

### 3. MMRecBlock (Her Blok - 24 Katman)

#### 3.1 QKVZ ProjeksiyonlarÄ±
- **W_q**: 16,781,312 (4,096 Ã— 4,096 + bias)
- **W_k**: 16,781,312 (4,096 Ã— 4,096 + bias)
- **W_v**: 16,781,312 (4,096 Ã— 4,096 + bias)
- **W_z**: 16,781,312 (4,096 Ã— 4,096 + bias)
- **W_g**: 16,781,312 (4,096 Ã— 4,096 + bias)
- **Subtotal**: 83,906,560

#### 3.2 MDI (Memory Decay/Integration)
- **Parametreler**: 50,345,984
- **BileÅŸenler**:
  - W_g: 4,096 Ã— 2 Ã— 4,096 = 33,554,432 (gating)
  - W_gamma: 4,096 Ã— 1,024 + 1,024 Ã— 4,096 = 8,388,608 (decay)
  - W_context: 4,096 Ã— 1,024 + 1,024 Ã— 4,096 = 8,388,608 (context modulation)

#### 3.3 MultiMemoryAttention
- **Parametreler**: 33,562,624
- **BileÅŸenler**:
  - W_q: 4,096 Ã— 4,096 = 16,781,312
  - W_o: 4,096 Ã— 4,096 = 16,781,312

#### 3.4 FFN (Feed-Forward Network)
- **Parametreler**: 134,238,208
- **Hesaplama**: 
  - Up projection: `model_dim Ã— ffn_dim = 4,096 Ã— 16,384 = 67,108,864`
  - Down projection: `ffn_dim Ã— model_dim = 16,384 Ã— 4,096 = 67,108,864`
  - Bias: 16,384 + 4,096 = 20,480

#### 3.5 Normalization
- **Norm1**: 4,096 (RMSNorm weight)
- **Norm2**: 4,096 (RMSNorm weight)
- **Subtotal**: 8,192

#### 3.6 Block ToplamÄ±
- **Her Blok**: 302,061,568 parametre
- **24 Blok**: 7,249,477,632 parametre

### 4. Final Normalization
- **Parametreler**: 4,096 (RMSNorm weight)

---

## ğŸ“ˆ KarÅŸÄ±laÅŸtÄ±rma

| Model | Parametre SayÄ±sÄ± | Fark |
|-------|------------------|------|
| LLaMA 7B | ~7.0B | - |
| MM-Rec 7B | 7.38B | +0.38B (+5.4%) |

**Not**: MM-Rec'in fazladan parametreleri:
- MDI modÃ¼lÃ¼ (decay ve gating mekanizmasÄ±)
- MultiMemoryAttention (O(M) complexity attention)
- Ekstra gating projeksiyonlarÄ± (W_g)

---

## ğŸ§® Parametre Hesaplama FormÃ¼lleri

### Embedding
```
params = vocab_size Ã— model_dim
      = 32,000 Ã— 4,096
      = 131,072,000
```

### QKVZ ProjeksiyonlarÄ± (her biri)
```
params = model_dim Ã— model_dim + model_dim (bias)
      = 4,096 Ã— 4,096 + 4,096
      = 16,781,312
```

### FFN
```
params = (model_dim Ã— ffn_dim) + (ffn_dim Ã— model_dim) + biases
      = (4,096 Ã— 16,384) + (16,384 Ã— 4,096) + (16,384 + 4,096)
      = 67,108,864 + 67,108,864 + 20,480
      = 134,238,208
```

### Toplam (Embedding hariÃ§)
```
total = (block_params Ã— num_layers) + final_norm
      = (302,061,568 Ã— 24) + 4,096
      = 7,249,477,632 + 4,096
      = 7,249,481,728
```

### Toplam (Embedding dahil)
```
total = embedding + blocks + final_norm
      = 131,072,000 + 7,249,481,728
      = 7,380,553,728
```

---

## ğŸ’¾ Bellek Gereksinimleri

### Model AÄŸÄ±rlÄ±klarÄ±
- **BF16**: 13.75 GB
- **FP32**: 27.49 GB

### EÄŸitim (Gradient + Optimizer States)
- **AdamW Optimizer**: ~2x model size (FP32)
  - Momentum: 27.49 GB
  - Variance: 27.49 GB
  - **Toplam**: ~55 GB (FP32)
- **Gradient**: 27.49 GB (FP32)
- **Activations**: Sequence length'e baÄŸlÄ± (chunking ile azaltÄ±labilir)

### Toplam EÄŸitim BelleÄŸi (Tahmini)
- **FP32 Training**: ~82 GB (model + gradients + optimizer)
- **Mixed Precision (BF16)**: ~55 GB (model BF16, optimizer FP32)

---

## ğŸ“ Notlar

1. **Tied Weights**: Embedding ve LM Head aynÄ± aÄŸÄ±rlÄ±klarÄ± paylaÅŸÄ±yor (weight tying). Bu, parametre sayÄ±sÄ±nÄ± azaltÄ±r ve model performansÄ±nÄ± artÄ±rÄ±r.

2. **MDI Parametreleri**: MM-Rec'in benzersiz Ã¶zelliÄŸi olan Memory Decay/Integration modÃ¼lÃ¼, ekstra parametreler ekler (~50M per block).

3. **Attention Parametreleri**: MultiMemoryAttention, standart Transformer attention'dan daha az parametre kullanÄ±r Ã§Ã¼nkÃ¼ Key ve Value projeksiyonlarÄ± HDS memory'den gelir (O(M) complexity).

4. **FFN Boyutu**: FFN dimension, model_dim'den 4x daha bÃ¼yÃ¼k (16,384 vs 4,096), bu standart LLM pratiÄŸidir.

---

## ğŸ” DoÄŸrulama

Parametre sayÄ±sÄ± `model.get_num_params()` metodu ile doÄŸrulanmÄ±ÅŸtÄ±r:

```python
from mm_rec.model import MMRecModel

model = MMRecModel(
    vocab_size=32000,
    model_dim=4096,
    num_layers=24,
    num_heads=32,
    ffn_dim=16384
)

total_params = model.get_num_params()
print(f"Total: {total_params:,} ({total_params / 1e9:.2f}B)")
# Output: Total: 7,380,553,728 (7.38B)
```

---

**Son GÃ¼ncelleme**: 2025-12-08

