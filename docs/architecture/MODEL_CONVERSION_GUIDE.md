# MM-Rec Model Weight Conversion Guide

**Tarih**: 2025-12-08  
**AmaÃ§**: Mevcut LLM aÄŸÄ±rlÄ±klarÄ±nÄ± MM-Rec mimarisine dÃ¶nÃ¼ÅŸtÃ¼rme

---

## ğŸ¯ Genel BakÄ±ÅŸ

MM-Rec model converter, mevcut Transformer tabanlÄ± LLM aÄŸÄ±rlÄ±klarÄ±nÄ± (LLaMA, GPT, vb.) MM-Rec mimarisine dÃ¶nÃ¼ÅŸtÃ¼rmenizi saÄŸlar. Bu iÅŸlem **modeli Ã§alÄ±ÅŸtÄ±rmadan**, sadece aÄŸÄ±rlÄ±klarÄ± analiz ederek yapÄ±lÄ±r.

### Desteklenen Ã–zellikler

âœ… **Uyumlu BileÅŸenler** (DoÄŸrudan Transfer):
- Embedding layer
- LM Head (output projection)
- FFN (Feed-Forward Network)
- Layer Normalization
- BazÄ± attention projeksiyonlarÄ± (Q, O)

âš ï¸ **Yeni BileÅŸenler** (Rastgele BaÅŸlatma):
- MDI (Memory Decay/Integration) - MM-Rec'e Ã¶zgÃ¼
- HDS (Hierarchical Data Structure) - MM-Rec'e Ã¶zgÃ¼
- Associative Scan - MM-Rec'e Ã¶zgÃ¼
- MultiMemoryAttention (tam olarak farklÄ±)
- Gating projeksiyonlarÄ± (W_g, W_z)

---

## ğŸ“‹ KullanÄ±m

### 1. Komut SatÄ±rÄ± KullanÄ±mÄ±

```bash
python -m mm_rec.scripts.convert_weights \
    --source llama-7b.pt \
    --output mmrec-7b-converted.pt \
    --vocab_size 32000 \
    --model_dim 4096 \
    --num_layers 24 \
    --num_heads 32
```

### 2. Python API KullanÄ±mÄ±

```python
from mm_rec.model import MMRecModel
from mm_rec.utils.model_converter import convert_model_weights

# MM-Rec model oluÅŸtur
model = MMRecModel(
    vocab_size=32000,
    model_dim=4096,
    num_layers=24,
    num_heads=32,
    ffn_dim=16384
)

# AÄŸÄ±rlÄ±klarÄ± dÃ¶nÃ¼ÅŸtÃ¼r
converted_weights, report = convert_model_weights(
    source_checkpoint_path='llama-7b.pt',
    target_model=model,
    output_path='mmrec-7b-converted.pt',
    strict=False,  # Missing keys iÃ§in hata verme
    initialize_new=True  # Yeni bileÅŸenleri baÅŸlat
)

# Model'e yÃ¼kle
model.load_state_dict(converted_weights, strict=False)
```

---

## ğŸ” DÃ¶nÃ¼ÅŸÃ¼m SÃ¼reci

### AdÄ±m 1: Kaynak Model Analizi

Converter, kaynak modelin state_dict'ini analiz eder:
- Model tipi (LLaMA, GPT, vb.)
- Vocab size
- Model dimension
- Layer sayÄ±sÄ±
- Attention head sayÄ±sÄ±
- FFN dimension

### AdÄ±m 2: Uyumlu AÄŸÄ±rlÄ±k EÅŸleÅŸtirme

Converter, kaynak ve hedef modeller arasÄ±nda uyumlu aÄŸÄ±rlÄ±klarÄ± bulur:

| Kaynak (LLaMA) | Hedef (MM-Rec) | Uyumluluk |
|----------------|----------------|-----------|
| `embed_tokens.weight` | `embedding.weight` | âœ… 100% |
| `lm_head.weight` | `lm_head.weight` | âœ… 100% |
| `layers.{i}.norm.weight` | `blocks.{i}.norm1.weight` | âœ… 100% |
| `layers.{i}.mlp.up_proj.weight` | `blocks.{i}.ffn.0.weight` | âœ… 100% |
| `layers.{i}.mlp.down_proj.weight` | `blocks.{i}.ffn.3.weight` | âœ… 100% |
| `layers.{i}.attention.q_proj.weight` | `blocks.{i}.multi_mem_attention.W_q.weight` | âœ… 100% |
| `layers.{i}.attention.o_proj.weight` | `blocks.{i}.multi_mem_attention.W_o.weight` | âœ… 100% |

### AdÄ±m 3: Yeni BileÅŸenlerin BaÅŸlatÄ±lmasÄ±

MM-Rec'e Ã¶zgÃ¼ bileÅŸenler rastgele baÅŸlatÄ±lÄ±r:
- **MDI**: Xavier uniform initialization
- **HDS**: Memory banks sÄ±fÄ±rdan baÅŸlatÄ±lÄ±r
- **Associative Scan**: Parametresiz (kernel-based)
- **W_g, W_z**: Benzer aÄŸÄ±rlÄ±klardan initialize edilebilir

### AdÄ±m 4: DÃ¶nÃ¼ÅŸÃ¼m Raporu

Converter, detaylÄ± bir rapor oluÅŸturur:
- DÃ¶nÃ¼ÅŸtÃ¼rÃ¼len aÄŸÄ±rlÄ±k sayÄ±sÄ±
- Eksik aÄŸÄ±rlÄ±klar
- Shape uyumsuzluklarÄ±
- Uyumluluk skorlarÄ±

---

## ğŸ“Š Ã–rnek DÃ¶nÃ¼ÅŸÃ¼m Raporu

```json
{
  "total_keys": 245,
  "converted_keys": 180,
  "missing_keys": 65,
  "new_keys": [
    "blocks.0.mdi.W_g.weight",
    "blocks.0.mdi.W_gamma.0.weight",
    "blocks.0.multi_mem_attention.W_q.weight"
  ],
  "source_analysis": {
    "model_type": "llama",
    "vocab_size": 32000,
    "model_dim": 4096,
    "num_layers": 24,
    "num_heads": 32,
    "ffn_dim": 11008
  },
  "compatibility_scores": {
    "embedding.weight": 1.0,
    "blocks.0.ffn.0.weight": 1.0,
    "blocks.0.multi_mem_attention.W_q.weight": 0.8
  }
}
```

---

## âš ï¸ Ã–nemli Notlar

### 1. Partial Loading

MM-Rec, Transformer'dan farklÄ± bir mimariye sahip olduÄŸu iÃ§in **tam uyumluluk beklenmez**. Genellikle:
- **~70-80%** aÄŸÄ±rlÄ±k transfer edilebilir
- **~20-30%** yeni bileÅŸenler rastgele baÅŸlatÄ±lÄ±r

### 2. Fine-tuning GerekliliÄŸi

DÃ¶nÃ¼ÅŸtÃ¼rÃ¼len model, **mutlaka fine-tuning** gerektirir Ã§Ã¼nkÃ¼:
- Yeni bileÅŸenler (MDI, HDS) rastgele baÅŸlatÄ±lmÄ±ÅŸtÄ±r
- Attention mekanizmasÄ± farklÄ±dÄ±r (MultiMemoryAttention)
- Core formula (h_t = z_t âŠ™ Ïƒ(W_g h_{t-1}) + Î³ âŠ™ h_{t-1}) yeni bir yapÄ±dÄ±r

### 3. Transfer Learning Stratejisi

Ã–nerilen yaklaÅŸÄ±m:
1. **AÄŸÄ±rlÄ±klarÄ± transfer et** (uyumlu olanlar)
2. **KÄ±sa bir fine-tuning** yap (1-5 epoch)
3. **Yeni bileÅŸenleri Ã¶ÄŸrenmesine izin ver**

### 4. Desteklenen Formatlar

- `.pt` (PyTorch checkpoint)
- `.pth` (PyTorch checkpoint)
- `.safetensors` (SafeTensors format, `safetensors` library gerekli)

---

## ğŸ”§ GeliÅŸmiÅŸ KullanÄ±m

### Ã–zel Key Mapping

EÄŸer kaynak modeliniz farklÄ± bir key yapÄ±sÄ±na sahipse, `ModelWeightConverter` sÄ±nÄ±fÄ±nÄ± extend edebilirsiniz:

```python
from mm_rec.utils.model_converter import ModelWeightConverter

class CustomConverter(ModelWeightConverter):
    def _keys_match(self, source_pattern, source_key, target_key):
        # Ã–zel eÅŸleÅŸtirme mantÄ±ÄŸÄ±
        # ...
        return super()._keys_match(source_pattern, source_key, target_key)
```

### Shape Transformation

BazÄ± aÄŸÄ±rlÄ±klar shape dÃ¶nÃ¼ÅŸÃ¼mÃ¼ gerektirebilir (transpose, reshape):

```python
# Converter otomatik olarak transpose yapar
# EÄŸer source: [4096, 4096] ve target: [4096, 4096] ise
# Otomatik olarak .t() uygulanÄ±r
```

---

## ğŸ“ˆ Beklenen SonuÃ§lar

### BaÅŸarÄ±lÄ± DÃ¶nÃ¼ÅŸÃ¼m Ä°ÅŸaretleri

âœ… **70%+ aÄŸÄ±rlÄ±k transferi**: Ä°yi bir baÅŸlangÄ±Ã§ noktasÄ±  
âœ… **Embedding ve FFN transferi**: Temel Ã¶zellikler korunur  
âœ… **Shape uyumluluÄŸu**: Ã‡oÄŸu aÄŸÄ±rlÄ±k uyumlu

### Dikkat Edilmesi Gerekenler

âš ï¸ **<50% transfer**: Model yapÄ±sÄ± Ã§ok farklÄ± olabilir  
âš ï¸ **Ã‡ok fazla shape mismatch**: Model boyutlarÄ± uyumsuz  
âš ï¸ **Yeni bileÅŸenler Ã§ok fazla**: Fine-tuning daha uzun sÃ¼rebilir

---

## ğŸš€ Sonraki AdÄ±mlar

1. **DÃ¶nÃ¼ÅŸtÃ¼rÃ¼len modeli yÃ¼kle**:
   ```python
   model.load_state_dict(torch.load('mmrec-7b-converted.pt'), strict=False)
   ```

2. **Fine-tuning baÅŸlat**:
   ```bash
   python -m mm_rec.scripts.train \
       --checkpoint mmrec-7b-converted.pt \
       --num_steps 10000
   ```

3. **PerformansÄ± deÄŸerlendir**:
   - Transfer edilen aÄŸÄ±rlÄ±klar sayesinde daha hÄ±zlÄ± convergence
   - Yeni bileÅŸenler Ã¶ÄŸrenilene kadar performans dÃ¼ÅŸÃ¼k olabilir

---

## ğŸ“ Ã–rnek Senaryolar

### Senaryo 1: LLaMA 7B â†’ MM-Rec 7B

```bash
python -m mm_rec.scripts.convert_weights \
    --source llama-7b/consolidated.00.pth \
    --output mmrec-7b-from-llama.pt \
    --vocab_size 32000 \
    --model_dim 4096 \
    --num_layers 24 \
    --num_heads 32
```

**Beklenen SonuÃ§**: ~75% aÄŸÄ±rlÄ±k transferi

### Senaryo 2: GPT-2 â†’ MM-Rec

```bash
python -m mm_rec.scripts.convert_weights \
    --source gpt2.pt \
    --output mmrec-from-gpt2.pt \
    --vocab_size 50257 \
    --model_dim 768 \
    --num_layers 12 \
    --num_heads 12
```

**Beklenen SonuÃ§**: ~60% aÄŸÄ±rlÄ±k transferi (daha kÃ¼Ã§Ã¼k model)

---

## â“ SÄ±k Sorulan Sorular

**S: TÃ¼m aÄŸÄ±rlÄ±klar transfer edilebilir mi?**  
C: HayÄ±r. MM-Rec'in benzersiz bileÅŸenleri (MDI, HDS) yeni baÅŸlatÄ±lÄ±r.

**S: Transfer edilen model hemen Ã§alÄ±ÅŸÄ±r mÄ±?**  
C: Evet, ama performans dÃ¼ÅŸÃ¼k olabilir. Fine-tuning Ã¶nerilir.

**S: Hangi modeller desteklenir?**  
C: LLaMA, GPT-2, GPT-Neo gibi Transformer tabanlÄ± modeller. Key pattern'leri otomatik tespit edilir.

**S: Model boyutlarÄ± farklÄ±ysa ne olur?**  
C: Uyumlu olanlar transfer edilir, uyumsuz olanlar rastgele baÅŸlatÄ±lÄ±r.

---

**Son GÃ¼ncelleme**: 2025-12-08

