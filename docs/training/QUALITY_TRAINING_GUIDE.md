# Kaliteli EÄŸitim Rehberi

**Tarih**: 2025-01-27  
**AmaÃ§**: En kÃ¼Ã§Ã¼k modelin bile kaliteli ve saÄŸlam temellerle eÄŸitilmesi

---

## ğŸ¯ Kaliteli EÄŸitim Prensipleri

### 1. GerÃ§ek Data KullanÄ±mÄ±
- âŒ **Ã–nceki**: SimÃ¼le edilmiÅŸ random data
- âœ… **Åimdi**: GerÃ§ek text data (character-level tokenization)
- âœ… Sample corpus ile baÅŸla, gerÃ§ek dataset'e geÃ§

### 2. Validation ve Evaluation
- âœ… Validation split (10% default)
- âœ… Perplexity metrikleri
- âœ… Accuracy metrikleri
- âœ… Early stopping (patience: 5 epochs)

### 3. Best Model Saving
- âœ… En iyi validation loss'a gÃ¶re model kaydetme
- âœ… Checkpoint'ler: step-based + best model

### 4. Proper Hyperparameters
- âœ… Learning rate: 3e-4 (standart)
- âœ… Warmup steps: 100
- âœ… Weight decay: 0.1
- âœ… Gradient clipping: 1.0

---

## ğŸ“‹ Yeni Ã–zellikler

### 1. GerÃ§ek Text Data Loader

**Dosya**: `mm_rec/data/text_data_loader.py`

**Ã–zellikler**:
- Character-level tokenization
- Sliding window sequence generation
- Train/validation split
- Sample corpus oluÅŸturma

**KullanÄ±m**:
```python
from mm_rec.data.text_data_loader import create_data_loaders

train_loader, val_loader, tokenizer = create_data_loaders(
    train_texts=train_texts,
    val_texts=val_texts,
    vocab_size=5000,
    seq_len=512,
    batch_size=4
)
```

### 2. Evaluation Metrikleri

**Dosya**: `mm_rec/training/evaluation.py`

**Metrikler**:
- **Loss**: Cross-entropy loss
- **Perplexity**: exp(loss)
- **Accuracy**: Token-level accuracy

**KullanÄ±m**:
```python
from mm_rec.training.evaluation import evaluate_model, print_evaluation_metrics

val_metrics = evaluate_model(
    model=model,
    data_loader=val_loader,
    criterion=criterion,
    device=device
)
print_evaluation_metrics(val_metrics)
```

### 3. Early Stopping

**Ã–zellikler**:
- Validation loss'a gÃ¶re early stopping
- Patience: 5 epochs (default)
- Best model otomatik kaydedilir

---

## ğŸš€ KullanÄ±m

### Temel EÄŸitim (Sample Corpus ile)

```bash
python mm_rec/scripts/train_base_model.py \
    --config tiny \
    --epochs 10 \
    --batch-size 4 \
    --seq-len 256 \
    --use-sample-corpus
```

### GerÃ§ek Dataset ile

```bash
# 1. Dataset hazÄ±rla (text dosyalarÄ± bir dizinde)
mkdir -p data/train
# text dosyalarÄ±nÄ± data/train/ dizinine koy

# 2. EÄŸitimi baÅŸlat
python mm_rec/scripts/train_base_model.py \
    --config tiny \
    --data-dir data/train \
    --epochs 20 \
    --val-split 0.1 \
    --early-stopping-patience 5 \
    --save-best-model
```

### Parametreler

- `--config`: Model konfigÃ¼rasyonu (tiny, mini, small, etc.)
- `--data-dir`: Dataset dizini (None = sample corpus)
- `--use-sample-corpus`: Sample corpus kullan (default: True)
- `--val-split`: Validation split ratio (default: 0.1)
- `--early-stopping-patience`: Early stopping patience (default: 5)
- `--save-best-model`: Best model kaydet (default: True)

---

## ğŸ“Š Ã‡Ä±ktÄ±lar

### Checkpoint'ler

1. **Step-based checkpoints**: `checkpoints/{config}/checkpoint_step_{step}.pt`
   - Her 100 step'te bir kaydedilir
   - Training devam ederken kullanÄ±lÄ±r

2. **Best model**: `checkpoints/{config}/best_model.pt`
   - En iyi validation loss'a sahip model
   - Early stopping veya final checkpoint olarak kullanÄ±lÄ±r

3. **Final checkpoint**: `checkpoints/{config}/final_checkpoint.pt`
   - TÃ¼m epoch'lar tamamlandÄ±ÄŸÄ±nda kaydedilir

### Log Ã‡Ä±ktÄ±larÄ±

```
ğŸ“š Preparing data...
âœ… Using sample corpus: checkpoints/sample_corpus.txt
âœ… Vocabulary built: 150 tokens
âœ… Train dataset: 1000 sequences
âœ… Validation dataset: 100 sequences

ğŸš€ Training started...

Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:30<00:00, loss=8.234, lr=3.00e-04]

ğŸ“Š Epoch 1 completed - Avg Loss: 8.234

Validation Metrics:
  Loss: 7.891
  Perplexity: 2654.23
  Accuracy: 12.34%
  Batches: 25

ğŸ’¾ Best model saved: checkpoints/tiny/best_model.pt
```

---

## âœ… Kalite Kontrol Listesi

### EÄŸitim Ã–ncesi
- [ ] Dataset hazÄ±r (gerÃ§ek text veya sample corpus)
- [ ] Validation split belirlendi
- [ ] Hyperparameter'lar ayarlandÄ±
- [ ] Model konfigÃ¼rasyonu doÄŸru

### EÄŸitim SÄ±rasÄ±nda
- [ ] Loss dÃ¼ÅŸÃ¼yor mu?
- [ ] Validation loss training loss'tan dÃ¼ÅŸÃ¼k mÃ¼?
- [ ] Perplexity makul deÄŸerlerde mi? (Ã§ok yÃ¼ksek deÄŸil)
- [ ] Accuracy artÄ±yor mu?
- [ ] Early stopping Ã§alÄ±ÅŸÄ±yor mu?

### EÄŸitim SonrasÄ±
- [ ] Best model kaydedildi mi?
- [ ] Final checkpoint kaydedildi mi?
- [ ] Evaluation metrikleri raporlandÄ± mÄ±?
- [ ] Model test edilebilir durumda mÄ±?

---

## ğŸ” Sorun Giderme

### Loss Ã‡ok YÃ¼ksek
- Learning rate'i dÃ¼ÅŸÃ¼r (1e-4)
- Warmup steps'i artÄ±r (200)
- Batch size'Ä± artÄ±r

### Loss DÃ¼ÅŸmÃ¼yor
- Model Ã§ok kÃ¼Ã§Ã¼k olabilir (config'i kontrol et)
- Data yeterli deÄŸil (daha fazla text ekle)
- Learning rate Ã§ok dÃ¼ÅŸÃ¼k (3e-4'e Ã§Ä±kar)

### Validation Loss ArtÄ±yor (Overfitting)
- Dropout'u artÄ±r (0.2)
- Weight decay'Ä± artÄ±r (0.2)
- Early stopping patience'Ä± azalt (3)

### Memory HatasÄ±
- Batch size'Ä± azalt (2)
- Sequence length'i azalt (256)
- Gradient checkpointing kullan

---

## ğŸ“ˆ Ä°yileÅŸtirme Ã–nerileri

### KÄ±sa Vadede
1. âœ… GerÃ§ek text data loader (tamamlandÄ±)
2. âœ… Validation ve evaluation (tamamlandÄ±)
3. âœ… Early stopping (tamamlandÄ±)
4. â³ GerÃ§ek dataset entegrasyonu (OpenWebText, etc.)

### Orta Vadede
1. Word-level tokenization (BPE, SentencePiece)
2. Daha fazla evaluation metrikleri (BLEU, ROUGE)
3. Learning rate scheduling iyileÅŸtirmeleri
4. Mixed precision training

### Uzun Vadede
1. Distributed training desteÄŸi
2. Advanced data augmentation
3. Curriculum learning
4. Multi-task learning

---

**HazÄ±rlayan**: MM-Rec Training Team  
**Tarih**: 2025-01-27  
**Durum**: âœ… Kaliteli eÄŸitim altyapÄ±sÄ± hazÄ±r
