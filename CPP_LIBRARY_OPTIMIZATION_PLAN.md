# ğŸš€ CPU iÃ§in C++ KÃ¼tÃ¼phanesi - Maksimum Optimizasyon PlanÄ±

**Tarih**: 2025-01-27  
**Hedef**: CPU'da maksimum performans iÃ§in native C++ kÃ¼tÃ¼phanesi  
**Durum**: Mevcut C++ kodu var ama optimize edilmeli

---

## ğŸ“Š Mevcut Durum Analizi

### âœ… Mevcut C++ DosyalarÄ±
1. **`associative_scan_cpu.cpp`** âœ…
   - AVX optimizasyonlarÄ± var (kÄ±smi)
   - OpenMP paralelizasyonu var
   - **Sorun**: exp() fonksiyonu scalar (SIMD deÄŸil)
   - **Sorun**: Blelloch scan tam implement edilmemiÅŸ (sequential)

2. **`mm_rec_block_cpp.cpp`** âœ…
   - Basit sequential loop var
   - **Sorun**: Fused kernel yok
   - **Sorun**: SIMD optimizasyonlarÄ± yok
   - **Sorun**: OpenMP kullanÄ±lmÄ±yor

### âš ï¸ Eksikler
1. **Vectorized exp/log fonksiyonlarÄ±** (SIMD)
2. **Tam Blelloch parallel scan** (ÅŸu an sequential)
3. **Core recurrence fused kernel**
4. **MDI optimized kernel**
5. **Attention CPU kernel**
6. **Memory alignment optimizasyonlarÄ±**

---

## ğŸ¯ Kritik Optimizasyonlar

### 1. Associative Scan - EN KRÄ°TÄ°K â­â­â­â­â­

#### Mevcut Sorunlar
```cpp
// Mevcut: exp() scalar - SIMD deÄŸil
for (int i = 0; i < 8; ++i) {
    result_arr[i] = max_arr[i] + std::log1p(std::exp(-clamped_arr[i]));
}
```

#### Ä°yileÅŸtirme: Vectorized Exp/Log
```cpp
// AVX-512 vectorized exp approximation
__m512 vectorized_exp_avx512(__m512 x) {
    // Fast exp approximation using polynomial
    // vexp â‰ˆ 1 + x + xÂ²/2 + xÂ³/6 + ...
    // Optimized for [-20, 0] range
}
```

#### Ä°yileÅŸtirme: Tam Blelloch Scan
```cpp
// Mevcut: Sequential scan (O(n))
// Ä°yileÅŸtirme: Parallel Blelloch scan (O(log n) depth)
void blelloch_scan_parallel(
    float* input,
    float* output,
    int n,
    int num_threads
) {
    // Up-sweep phase (parallel reduction tree)
    // Down-sweep phase (parallel prefix propagation)
}
```

**Beklenen HÄ±zlanma**: 10-20x

---

### 2. Core Recurrence Formula - KRÄ°TÄ°K â­â­â­â­

#### Mevcut Durum
- PyTorch operations (F.linear, matmul, sigmoid)
- Multiple small operations
- Python overhead

#### C++ Fused Kernel
```cpp
void core_recurrence_fused_avx512(
    const float* z_t,        // [batch, seq_len, hidden_dim]
    const float* h_prev,     // [batch, seq_len, hidden_dim]
    const float* W_g,        // [hidden_dim, hidden_dim]
    const float* gamma,       // [batch, seq_len, hidden_dim]
    float* h_t,              // Output
    int batch_size,
    int seq_len,
    int hidden_dim
) {
    // Fused: h_t = z_t âŠ™ Ïƒ(W_g @ h_prev) + Î³ âŠ™ h_prev
    // All operations in single kernel:
    // 1. Matrix-vector multiply (MKL)
    // 2. Vectorized sigmoid (SIMD)
    // 3. Element-wise operations (SIMD)
}
```

**Beklenen HÄ±zlanma**: 5-10x

---

### 3. MDI Optimized - Ã–NEMLÄ° â­â­â­

#### Fused MDI Kernel
```cpp
void mdi_update_fused_simd(
    const float* h_new,
    const float* h_old,
    const float* gamma,
    const float* gate,
    float* h_updated,
    int n
) {
    // SIMD: h_updated = gate âŠ™ h_new + (1-gate) âŠ™ h_old + Î³ âŠ™ h_old
    // AVX-512: 16 floats at once
}
```

**Beklenen HÄ±zlanma**: 3-5x

---

## ğŸ—ï¸ Yeni C++ KÃ¼tÃ¼phanesi Mimarisi

### KlasÃ¶r YapÄ±sÄ±
```
mm_rec/cpp/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ associative_scan_cpu.cpp          # âœ… Mevcut (geliÅŸtirilmeli)
â”‚   â”‚   â”œâ”€â”€ associative_scan_cpu.h
â”‚   â”‚   â”œâ”€â”€ log_sum_exp_simd.cpp             # ğŸ†• Vectorized LSE
â”‚   â”‚   â”œâ”€â”€ log_sum_exp_simd.h
â”‚   â”‚   â”œâ”€â”€ exp_log_simd.cpp                 # ğŸ†• Vectorized exp/log
â”‚   â”‚   â”œâ”€â”€ exp_log_simd.h
â”‚   â”‚   â”œâ”€â”€ blelloch_scan_parallel.cpp        # ğŸ†• True parallel scan
â”‚   â”‚   â””â”€â”€ blelloch_scan_parallel.h
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â”œâ”€â”€ mm_rec_block_cpp.cpp              # âœ… Mevcut (geliÅŸtirilmeli)
â”‚   â”‚   â”œâ”€â”€ core_recurrence_fused.cpp         # ğŸ†• Fused kernel
â”‚   â”‚   â”œâ”€â”€ core_recurrence_fused.h
â”‚   â”‚   â”œâ”€â”€ mdi_cpu_optimized.cpp             # ğŸ†• MDI SIMD
â”‚   â”‚   â””â”€â”€ mdi_cpu_optimized.h
â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â”œâ”€â”€ attention_cpu.cpp                 # ğŸ†• CPU attention
â”‚   â”‚   â”œâ”€â”€ attention_cpu.h
â”‚   â”‚   â””â”€â”€ softmax_simd.cpp                 # ğŸ†• SIMD softmax
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ simd_utils.cpp                    # ğŸ†• SIMD helpers
â”‚   â”‚   â”œâ”€â”€ simd_utils.h
â”‚   â”‚   â”œâ”€â”€ memory_utils.cpp                  # ğŸ†• Alignment, prefetch
â”‚   â”‚   â””â”€â”€ thread_pool.cpp                  # ğŸ†• Custom thread pool
â”‚   â””â”€â”€ bindings/
â”‚       â”œâ”€â”€ python_bindings.cpp               # PyTorch extension
â”‚       â””â”€â”€ python_bindings.h
â”œâ”€â”€ include/
â”‚   â””â”€â”€ mm_rec_cpp.h                         # Public API
â”œâ”€â”€ CMakeLists.txt                            # CMake build
â”œâ”€â”€ setup.py                                  # âœ… Mevcut (gÃ¼ncellenmeli)
â””â”€â”€ tests/
    â”œâ”€â”€ test_associative_scan.cpp
    â”œâ”€â”€ test_core_recurrence.cpp
    â””â”€â”€ benchmark.cpp
```

---

## ğŸ”§ Kritik C++ ImplementasyonlarÄ±

### 1. Vectorized Exp/Log (SIMD)

```cpp
// mm_rec/cpp/src/core/exp_log_simd.cpp

#include <immintrin.h>
#include <cmath>

// Fast exp approximation for AVX-512
// Optimized for range [-20, 0] (Log-Sum-Exp use case)
__m512 vectorized_exp_avx512(__m512 x) {
    // Clamp to [-20, 0]
    __m512 x_clamped = _mm512_max_ps(x, _mm512_set1_ps(-20.0f));
    x_clamped = _mm512_min_ps(x_clamped, _mm512_set1_ps(0.0f));
    
    // Fast polynomial approximation: exp(x) â‰ˆ 1 + x + xÂ²/2 + xÂ³/6
    // For better accuracy: use Remez polynomial or lookup table
    __m512 one = _mm512_set1_ps(1.0f);
    __m512 x2 = _mm512_mul_ps(x_clamped, x_clamped);
    __m512 x3 = _mm512_mul_ps(x2, x_clamped);
    
    __m512 result = _mm512_fmadd_ps(
        x_clamped, _mm512_set1_ps(1.0f),
        one
    );
    result = _mm512_fmadd_ps(
        x2, _mm512_set1_ps(0.5f),
        result
    );
    result = _mm512_fmadd_ps(
        x3, _mm512_set1_ps(1.0f/6.0f),
        result
    );
    
    return result;
}

// Vectorized log1p for Log-Sum-Exp
__m512 vectorized_log1p_avx512(__m512 x) {
    // Fast log1p approximation
    // log1p(x) â‰ˆ x - xÂ²/2 + xÂ³/3 for small x
    // For larger x, use standard log(1+x)
    __m512 small = _mm512_cmp_ps_mask(x, _mm512_set1_ps(0.1f), _CMP_LT_OQ);
    
    __m512 x2 = _mm512_mul_ps(x, x);
    __m512 x3 = _mm512_mul_ps(x2, x);
    
    __m512 approx = _mm512_sub_ps(x, _mm512_mul_ps(x2, _mm512_set1_ps(0.5f)));
    approx = _mm512_add_ps(approx, _mm512_mul_ps(x3, _mm512_set1_ps(1.0f/3.0f)));
    
    // For larger values, use standard log
    __m512 one = _mm512_set1_ps(1.0f);
    __m512 standard = vectorized_log_avx512(_mm512_add_ps(one, x));
    
    return _mm512_mask_blend_ps(small, standard, approx);
}
```

### 2. Tam Blelloch Parallel Scan

```cpp
// mm_rec/cpp/src/core/blelloch_scan_parallel.cpp

#include <omp.h>
#include <immintrin.h>

void blelloch_scan_parallel_avx512(
    float* input,      // Input array [n]
    float* output,     // Output array [n]
    int n,             // Array size
    int num_threads    // Number of threads
) {
    omp_set_num_threads(num_threads);
    
    // Step 1: Up-sweep Phase (Reduction Tree)
    // Build reduction tree: O(log n) depth
    for (int stride = 1; stride < n; stride *= 2) {
        #pragma omp parallel for
        for (int i = stride; i < n; i += 2 * stride) {
            int left = i - stride;
            int right = i;
            
            // Vectorized log-sum-exp
            #ifdef __AVX512F__
            for (int j = 0; j < n - 15; j += 16) {
                __m512 vleft = _mm512_load_ps(&input[left * n + j]);
                __m512 vright = _mm512_load_ps(&input[right * n + j]);
                __m512 vresult = vectorized_log_sum_exp_avx512(vleft, vright);
                _mm512_store_ps(&output[right * n + j], vresult);
            }
            #endif
        }
    }
    
    // Step 2: Down-sweep Phase (Prefix Propagation)
    // Propagate prefixes: O(log n) depth
    output[n-1] = 0.0f;  // Set last element to identity
    
    for (int stride = n / 2; stride > 0; stride /= 2) {
        #pragma omp parallel for
        for (int i = stride; i < n; i += 2 * stride) {
            int left = i - stride;
            int right = i;
            
            // Vectorized log-sum-exp
            #ifdef __AVX512F__
            for (int j = 0; j < n - 15; j += 16) {
                __m512 vleft = _mm512_load_ps(&output[left * n + j]);
                __m512 vright = _mm512_load_ps(&output[right * n + j]);
                __m512 vresult = vectorized_log_sum_exp_avx512(vleft, vright);
                _mm512_store_ps(&output[right * n + j], vresult);
            }
            #endif
        }
    }
}
```

### 3. Core Recurrence Fused Kernel

```cpp
// mm_rec/cpp/src/blocks/core_recurrence_fused.cpp

#include <immintrin.h>
#include <mkl.h>  // Intel MKL

void core_recurrence_fused_avx512(
    const float* z_t,        // [batch, seq_len, hidden_dim]
    const float* h_prev,      // [batch, seq_len, hidden_dim]
    const float* W_g,         // [hidden_dim, hidden_dim]
    const float* gamma,       // [batch, seq_len, hidden_dim]
    float* h_t,               // Output [batch, seq_len, hidden_dim]
    int batch_size,
    int seq_len,
    int hidden_dim
) {
    // Fused kernel: h_t = z_t âŠ™ Ïƒ(W_g @ h_prev) + Î³ âŠ™ h_prev
    
    #pragma omp parallel for collapse(2)
    for (int b = 0; b < batch_size; b++) {
        for (int t = 0; t < seq_len; t++) {
            int base_idx = b * seq_len * hidden_dim + t * hidden_dim;
            
            // 1. Matrix-vector multiply: g = W_g @ h_prev
            float* gate = new float[hidden_dim];
            cblas_sgemv(
                CblasRowMajor, CblasNoTrans,
                hidden_dim, hidden_dim,
                1.0f, W_g, hidden_dim,
                &h_prev[base_idx], 1,
                0.0f, gate, 1
            );
            
            // 2. Vectorized sigmoid: Ïƒ(g)
            for (int d = 0; d < hidden_dim - 15; d += 16) {
                __m512 vg = _mm512_load_ps(&gate[d]);
                __m512 vneg = _mm512_mul_ps(vg, _mm512_set1_ps(-1.0f));
                __m512 vexp = vectorized_exp_avx512(vneg);
                __m512 vone = _mm512_set1_ps(1.0f);
                __m512 vsigmoid = _mm512_div_ps(vone, _mm512_add_ps(vone, vexp));
                _mm512_store_ps(&gate[d], vsigmoid);
            }
            
            // 3. Fused element-wise: z_t âŠ™ Ïƒ(g) + Î³ âŠ™ h_prev
            for (int d = 0; d < hidden_dim - 15; d += 16) {
                __m512 vz = _mm512_load_ps(&z_t[base_idx + d]);
                __m512 vg = _mm512_load_ps(&gate[d]);
                __m512 vh = _mm512_load_ps(&h_prev[base_idx + d]);
                __m512 vgamma = _mm512_load_ps(&gamma[base_idx + d]);
                
                // z_t âŠ™ Ïƒ(g)
                __m512 vzg = _mm512_mul_ps(vz, vg);
                
                // Î³ âŠ™ h_prev
                __m512 vgh = _mm512_mul_ps(vgamma, vh);
                
                // Sum
                __m512 vht = _mm512_add_ps(vzg, vgh);
                
                _mm512_store_ps(&h_t[base_idx + d], vht);
            }
            
            delete[] gate;
        }
    }
}
```

---

## ğŸ› ï¸ Build System Ä°yileÅŸtirmeleri

### setup.py GÃ¼ncellemeleri

```python
# Modern CPU optimizations
cxx_args = [
    '-O3',                    # Maximum optimization
    '-march=native',          # Auto-detect CPU features
    '-mtune=native',          # Tune for native CPU
    '-mavx2',                 # AVX2 (8 floats)
    '-mavx512f',              # AVX-512 (16 floats) - if available
    '-mavx512cd',             # AVX-512 conflict detection
    '-mfma',                  # Fused Multiply-Add
    '-fopenmp',               # OpenMP
    '-funroll-loops',         # Loop unrolling
    '-ffast-math',            # Fast math (careful with numerical stability)
    '-fno-math-errno',        # Don't set errno
    '-flto',                  # Link-time optimization
]
```

### CMakeLists.txt (Alternatif)

```cmake
# Detect CPU features
include(CheckCXXCompilerFlag)
check_cxx_compiler_flag("-mavx512f" COMPILER_SUPPORTS_AVX512)
check_cxx_compiler_flag("-mavx2" COMPILER_SUPPORTS_AVX2)

if(COMPILER_SUPPORTS_AVX512)
    add_definitions(-DUSE_AVX512)
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -mavx512f -mavx512cd")
elseif(COMPILER_SUPPORTS_AVX2)
    add_definitions(-DUSE_AVX2)
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -mavx2")
endif()

# OpenMP
find_package(OpenMP REQUIRED)
target_link_libraries(mm_rec_cpp OpenMP::OpenMP_CXX)

# MKL or OpenBLAS
find_package(MKL QUIET)
if(MKL_FOUND)
    target_link_libraries(mm_rec_cpp ${MKL_LIBRARIES})
else()
    find_package(OpenBLAS QUIET)
    if(OpenBLAS_FOUND)
        target_link_libraries(mm_rec_cpp ${OpenBLAS_LIBRARIES})
    endif()
endif()
```

---

## ğŸ“ˆ Beklenen Performans Ä°yileÅŸtirmeleri

### Associative Scan
- **Mevcut**: Sequential, scalar exp() â†’ ~1000ms
- **Optimized**: Parallel Blelloch, SIMD exp() â†’ ~50-100ms
- **HÄ±zlanma**: **10-20x** â­

### Core Recurrence
- **Mevcut**: PyTorch operations â†’ ~200ms
- **Optimized**: Fused kernel, SIMD â†’ ~20-40ms
- **HÄ±zlanma**: **5-10x** â­

### MDI
- **Mevcut**: PyTorch â†’ ~50ms
- **Optimized**: SIMD â†’ ~10-20ms
- **HÄ±zlanma**: **3-5x**

### Overall Training
- **Mevcut**: ~82s/step
- **Optimized**: ~10-15s/step
- **HÄ±zlanma**: **5-8x** â­â­â­

---

## ğŸ¯ Uygulama Ã–ncelikleri

### Faz 1: En Kritik (Hemen) â­â­â­â­â­
1. âœ… **Vectorized Exp/Log** (SIMD)
   - AVX-512 exp approximation
   - Vectorized log1p
   - **Beklenen**: 5-10x hÄ±zlanma (Associative Scan iÃ§in)

2. âœ… **Tam Blelloch Parallel Scan**
   - Up-sweep + Down-sweep
   - OpenMP paralelizasyonu
   - **Beklenen**: 5-10x hÄ±zlanma

**Toplam Associative Scan**: 10-20x hÄ±zlanma

### Faz 2: YÃ¼ksek Ã–ncelik â­â­â­â­
3. âœ… **Core Recurrence Fused Kernel**
   - Fused operations
   - SIMD sigmoid
   - MKL matmul
   - **Beklenen**: 5-10x hÄ±zlanma

### Faz 3: Orta Ã–ncelik â­â­â­
4. âœ… **MDI Optimized**
5. âœ… **Attention CPU**
6. âœ… **Memory utilities**

---

## ğŸ”§ Teknik Detaylar

### SIMD Seviyeleri (Auto-detect)
```cpp
// Runtime CPU feature detection
bool has_avx512() {
    return __builtin_cpu_supports("avx512f");
}

bool has_avx2() {
    return __builtin_cpu_supports("avx2");
}

// Use best available SIMD
if (has_avx512()) {
    // Use AVX-512 (16 floats)
} else if (has_avx2()) {
    // Use AVX2 (8 floats)
} else {
    // Scalar fallback
}
```

### Thread Stratejisi
```cpp
// Optimal thread count
int optimal_threads() {
    int cores = std::thread::hardware_concurrency();
    // Use 75% of cores (leave some for OS)
    return std::max(1, (cores * 3) / 4);
}

omp_set_num_threads(optimal_threads());
```

### Memory Alignment
```cpp
// 64-byte alignment (cache line)
alignas(64) float data[hidden_dim];

// Prefetch next cache line
__builtin_prefetch(&data[i + 64], 0, 3);
```

---

## ğŸ“ Uygulama AdÄ±mlarÄ±

### AdÄ±m 1: Vectorized Exp/Log (1-2 gÃ¼n)
1. AVX-512 exp approximation implementasyonu
2. Vectorized log1p
3. Test ve benchmark

### AdÄ±m 2: Blelloch Scan (2-3 gÃ¼n)
1. Up-sweep phase
2. Down-sweep phase
3. OpenMP paralelizasyonu
4. Test ve doÄŸrulama

### AdÄ±m 3: Core Recurrence (2-3 gÃ¼n)
1. Fused kernel implementasyonu
2. SIMD sigmoid
3. MKL entegrasyonu
4. Test

### AdÄ±m 4: Entegrasyon (1 gÃ¼n)
1. PyTorch C++ extension
2. Python bindings
3. Test ve benchmark

---

## ğŸ‰ SonuÃ§

**CPU iÃ§in C++ kÃ¼tÃ¼phanesi kritik ve mÃ¼mkÃ¼n!**

### Mevcut Durum
- âœ… C++ kodu var ama optimize edilmeli
- âš ï¸ Sequential scan, scalar exp()
- âš ï¸ Fused kernel yok

### Ä°yileÅŸtirme Potansiyeli
- âœ… **10-20x hÄ±zlanma** (Associative Scan)
- âœ… **5-10x hÄ±zlanma** (Core Recurrence)
- âœ… **Toplam 5-8x** training hÄ±zlanmasÄ±

### Ã–ncelik
1. **Vectorized Exp/Log** (en kritik)
2. **Blelloch Parallel Scan**
3. **Core Recurrence Fused**

**SonuÃ§**: CPU'da maksimum optimizasyon iÃ§in C++ kÃ¼tÃ¼phanesi **zorunlu** ve **mÃ¼mkÃ¼n**!

---

**Sonraki AdÄ±m**: Vectorized Exp/Log implementasyonu ile baÅŸla!
